<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>llm on Nicola Iarocci</title>
    <link>https://nicolaiarocci.com/tags/llm/</link>
    <description>Recent content in llm on Nicola Iarocci</description>
    <image>
      <title>Nicola Iarocci</title>
      <url>https://nicolaiarocci.com/images/avatar.png</url>
      <link>https://nicolaiarocci.com/images/avatar.png</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <copyright>Produced / Written / Maintained by [Nicola Iarocci](/) since 2010</copyright>
    <lastBuildDate>Sun, 07 Jan 2024 10:57:43 +0100</lastBuildDate>
    <atom:link href="https://nicolaiarocci.com/tags/llm/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Some hints about what the next year of AI looks like</title>
      <link>https://nicolaiarocci.com/some-hints-about-what-the-next-year-of-ai-looks-like/</link>
      <pubDate>Sun, 07 Jan 2024 10:57:43 +0100</pubDate>
      <guid>https://nicolaiarocci.com/some-hints-about-what-the-next-year-of-ai-looks-like/</guid>
      <description>Professor Ethan Mollick&amp;rsquo;s Signs and Portents analyzes what AI has achieved, what the effects have been so far, and what we might expect in 2024.
To ground ourselves, we can start with two quotes that should inform any estimates about the future. The first is Amara&amp;rsquo;s Law: &amp;ldquo;We tend to overestimate the effect of a technology in the short run and underestimate the effect in the long run.&amp;rdquo; Social change is slower than technological change.</description>
      <content:encoded><![CDATA[<p>Professor Ethan Mollick&rsquo;s <a href="https://www.oneusefulthing.org/p/signs-and-portents">Signs and
Portents</a> analyzes what AI
has achieved, what the effects have been so far, and what we might expect in
2024.</p>
<blockquote>
<p>To ground ourselves, we can start with two quotes that should inform any
estimates about the future. The first is Amara&rsquo;s Law: &ldquo;We tend to overestimate
the effect of a technology in the short run and underestimate the effect in the
long run.&rdquo; Social change is slower than technological change. We should not
expect to see immediate global effects of AI in a major way, no matter how fast
its adoption (and it is remarkably fast), yet we certainly will see it sooner
than many people think.</p>
</blockquote>
<p>It&rsquo;s an insightful read that pairs well with Simon Wilson&rsquo;s <a href="/stuff-we-figured-out-about-ai-in-2023/">2023
round-up</a>. Also, his 30-second fake
video of himself telling things he never told is impressive.</p>
]]></content:encoded>
    </item>
    <item>
      <title>Stuff we figured out about AI in 2023</title>
      <link>https://nicolaiarocci.com/stuff-we-figured-out-about-ai-in-2023/</link>
      <pubDate>Mon, 01 Jan 2024 11:46:22 +0100</pubDate>
      <guid>https://nicolaiarocci.com/stuff-we-figured-out-about-ai-in-2023/</guid>
      <description>Simon Wilson, who&amp;rsquo;s recently been my go-to person for all AI-related stuff, has an excellent 2023 AI round-up on his website.
2023 was the breakthrough year for Large Language Models (LLMs). I think it&amp;rsquo;s OK to call these AI—they&amp;rsquo;re the latest and (currently) most &amp;ldquo;interesting development in the academic field of Artificial Intelligence that dates back to the 1950s. Here&amp;rsquo;s my attempt to round up the highlights in one place!</description>
      <content:encoded><![CDATA[<p>Simon Wilson, who&rsquo;s recently been my go-to person for all AI-related stuff, has
an excellent <a href="https://simonwillison.net/2023/Dec/31/ai-in-2023/">2023 AI
round-up</a> on his website.</p>
<blockquote>
<p>2023 was the breakthrough year for Large Language Models (LLMs). I think it&rsquo;s
OK to call these AI—they&rsquo;re the latest and (currently) most &ldquo;interesting
development in the academic field of Artificial Intelligence that dates back to
the 1950s. Here&rsquo;s my attempt to round up the highlights in one place!</p>
</blockquote>
<p>The links contained within the post are also valuable. You may know Simon&rsquo;s
website if you are interested in LLMs and AI. If you don&rsquo;t, I suggest you start
following him, preferably via his RSS feed like real hackers do.</p>
]]></content:encoded>
    </item>
    <item>
      <title>Quoting Andrej Karpathy</title>
      <link>https://nicolaiarocci.com/quoting-andrej-karpathy/</link>
      <pubDate>Sat, 09 Dec 2023 07:05:25 +0100</pubDate>
      <guid>https://nicolaiarocci.com/quoting-andrej-karpathy/</guid>
      <description>I always struggle a bit with I&amp;rsquo;m asked about the &amp;ldquo;hallucination problem&amp;rdquo; in LLMs. Because, in some sense, hallucination is all LLMs do. They are dream machines.
We direct their dreams with prompts. The prompts start the dream, and based on the LLM&amp;rsquo;s hazy recollection of its training documents, most of the time the result goes someplace useful.
It&amp;rsquo;s only when the dreams go into deemed factually incorrect territory that we label it a &amp;ldquo;hallucination&amp;rdquo;.</description>
      <content:encoded><![CDATA[<blockquote>
<p>I always struggle a bit with I&rsquo;m asked about the &ldquo;hallucination problem&rdquo; in LLMs. Because, in some sense, hallucination is all LLMs do. They are dream machines.</p>
</blockquote>
<blockquote>
<p>We direct their dreams with prompts. The prompts start the dream, and based on the LLM&rsquo;s hazy recollection of its training documents, most of the time the result goes someplace useful.</p>
</blockquote>
<blockquote>
<p>It&rsquo;s only when the dreams go into deemed factually incorrect territory that we label it a &ldquo;hallucination&rdquo;. It looks like a bug, but it&rsquo;s just the LLM doing what it always does.</p>
</blockquote>
<p>&ndash; <a href="https://twitter.com/karpathy/status/1733299213503787018">Andrej Karpathy</a></p>
]]></content:encoded>
    </item>
    <item>
      <title>Intro to Large Language Models (video)</title>
      <link>https://nicolaiarocci.com/intro-to-large-language-models-video/</link>
      <pubDate>Fri, 24 Nov 2023 07:05:25 +0100</pubDate>
      <guid>https://nicolaiarocci.com/intro-to-large-language-models-video/</guid>
      <description>Andrej Karpathy has a very well-done Intro to Large Language Models video on YouTube. As a founding member and research scientist at OpenAI and with a two-year hiatus working on Tesla Autopilot, Karpathy is an authority in the field. He is also good at explaining hard things.
As a Kahneman reader, I appreciated the Thinking Fast and Slow analogy proposed at about half-length in the video: &amp;ldquo;System 1&amp;rdquo; (fast automatic thinking, rapid decisions) is where we&amp;rsquo;re now; &amp;ldquo;System 2&amp;rdquo; (rational, slow thinking, complex decisions) is LLMs next goal.</description>
      <content:encoded><![CDATA[<p>Andrej Karpathy has a very well-done <a href="https://youtu.be/zjkBMFhNj_g?si=5tJNFaDcK-FBWnWK">Intro to Large Language Models</a>
video on YouTube. As a founding member and research scientist at OpenAI and with a two-year hiatus working on Tesla
Autopilot, Karpathy is an authority in the field. He is also good at explaining hard things.</p>
<p>As a Kahneman reader, I appreciated the <em>Thinking Fast and Slow</em> analogy proposed at about half-length in the video:
&ldquo;System 1&rdquo; (fast automatic thinking, rapid decisions) is where we&rsquo;re now; &ldquo;System 2&rdquo; (rational, slow thinking, complex
decisions) is LLMs next goal. Also, I suspect Karpathy&rsquo;s intriguing idea of LLMs as the center of a new &ldquo;operating
system style&rdquo; is not too far off from what will emerge soon. The final segment on AI security and known attack vectors
(jailbreaking, prompt injection, data poisoning) is also super interesting.</p>
<p>On his website, Karpathy also has a promising <a href="https://karpathy.ai/zero-to-hero.html">zero-to-hero video series</a>, &ldquo;a
course on building neural networks from scratch, in code.&rdquo;</p>
]]></content:encoded>
    </item>
    <item>
      <title>AI-curated minimalist news</title>
      <link>https://nicolaiarocci.com/ai-curated-minimalist-news/</link>
      <pubDate>Wed, 03 May 2023 07:05:25 +0100</pubDate>
      <guid>https://nicolaiarocci.com/ai-curated-minimalist-news/</guid>
      <description>Minimalist News is the first LLM project that excites me but in a nervous way. Quoting the About page:
We only publish significant news. To find them we use AI (ChatGPT-4) to read and analyze 1000 top news every day. For each article it estimates magnitude, scale, potential and credibility. Then we combine these estimates to get the final Significance score from 0 to 10. And now the best part: We&amp;rsquo;ll only send you the news scored 6.</description>
      <content:encoded><![CDATA[<p><a href="https://www.newsminimalist.com">Minimalist News</a> is the first LLM project that excites me but in a nervous way. Quoting the About page:</p>
<blockquote>
<p>We only publish significant news. To find them we use AI (ChatGPT-4) to read and analyze 1000 top news every day. For
each article it estimates magnitude, scale, potential and credibility. Then we combine these estimates to get the
final Significance score from 0 to 10. And now the best part: We&rsquo;ll only send you the news scored 6.5 or higher.
Sometimes it&rsquo;s 5 articles, sometimes 2, sometimes 8. And sometimes — none at all. But one thing is constant — you can
be sure that you haven&rsquo;t missed anything important.</p>
</blockquote>
<p>The concept is brilliant and well executed, but I can&rsquo;t help but feel uncomfortable at the notion of an AI curating news
for me. Yet, this is the best use case for LLM/AI I&rsquo;ve seen until now.</p>
]]></content:encoded>
    </item>
  </channel>
</rss>
