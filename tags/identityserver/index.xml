<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>identityserver on Nicola Iarocci</title>
    <link>https://nicolaiarocci.com/tags/identityserver/</link>
    <description>Recent content in identityserver on Nicola Iarocci</description>
    <generator>Hugo -- 0.121.1</generator>
    <language>en</language>
    <copyright>Produced / Written / Maintained by Nicola Iarocci since 2010</copyright>
    <lastBuildDate>Thu, 12 Sep 2024 16:17:58 +0200</lastBuildDate>
    <atom:link href="https://nicolaiarocci.com/tags/identityserver/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Under ASP.NET 8, NGINX returns 502 Bad Gateway after authentication by IdentityServer</title>
      <link>https://nicolaiarocci.com/under-asp.net-8-nginx-returns-502-bad-gateway-after-authentication-by-identityserver/</link>
      <pubDate>Thu, 12 Sep 2024 16:17:58 +0200</pubDate>
      <guid>https://nicolaiarocci.com/under-asp.net-8-nginx-returns-502-bad-gateway-after-authentication-by-identityserver/</guid>
      <description>Today, I learned the hard way that NGINX has default buffer sizes, which can cause trouble in specific scenarios like mine.</description>
      <content:encoded><![CDATA[<p>Today, I learned the hard way that NGINX has small buffer sizes, which can cause trouble in specific scenarios like mine.</p>
<p>We have two ASP.NET 8 applications behind NGINX;Â one is a regular web app, and the other is an auth server built with Duende Identity Server. When the user attempts to log in to the web app, they are sent to the auth server; once logged in, they are sent back to the app, and that&rsquo;s when NGINX returns a weird 502 Bad Gateway. Upon inspecting the NGINX logs, I found the following error:</p>
<pre><code>upstream sent too big header while reading response header from upstream
</code></pre>
<p>Some digging revealed that NGINX has a pretty small default buffer size, and proxying auth requests (with their typically above-average-size headers) back and forth is prone to breaking that limit.</p>
<p>The fix is straightforward: raise those buffer sizes. Most blogs suggest doing that globally, but that seems overkill, as NGINX buffering can be configured per server and location blocks. You may want to keep the global defaults and only raise them (along with memory consumption) for locations that need it.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>  location {
</span></span><span style="display:flex;"><span>    ...
</span></span><span style="display:flex;"><span>    proxy_buffers 4 256k;
</span></span><span style="display:flex;"><span>    proxy_buffer_size 128k;
</span></span><span style="display:flex;"><span>    proxy_busy_buffers_size 256k;
</span></span><span style="display:flex;"><span>    ...
</span></span><span style="display:flex;"><span>  }
</span></span></code></pre></div><p>Mind you, the above values (that I found in almost every blog dealing with this problem) are likely too big and should be trimmed down. That&rsquo;s my next task, using <a href="https://www.getpagespeed.com/server-setup/nginx/tuning-proxy_buffer_size-in-nginx">this</a> excellent article for guidance.</p>
]]></content:encoded>
    </item>
  </channel>
</rss>
