---
date: 2023-12-09T07:05:25+01:00
title: "Quoting Andrej Karpathy"
share: false
tags: ["quotes", "llm", "ai"]
---
> I always struggle a bit with I'm asked about the "hallucination problem" in LLMs. Because, in some sense, hallucination is all LLMs do. They are dream machines. 

> We direct their dreams with prompts. The prompts start the dream, and based on the LLM's hazy recollection of its training documents, most of the time the result goes someplace useful. 

> It's only when the dreams go into deemed factually incorrect territory that we label it a "hallucination". It looks like a bug, but it's just the LLM doing what it always does.

-- [Andrej Karpathy](https://twitter.com/karpathy/status/1733299213503787018)



 [rss]: https://nicolaiarocci.com/index.xml
 [m]: https://fosstodon.org/@nicola
 [nl]: https://buttondown.email/nicolaiarocci
